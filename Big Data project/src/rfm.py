# -*- coding: utf-8 -*-
"""rfm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jta4dTcHap1iHcEpmH_xVwjg_CZJB7s9
"""

import argparse
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, max as _max, to_date, datediff, lit
from pyspark.sql.utils import AnalysisException


def compute_rfm(parquet_path: str, out_xlsx: str, reference_date: str):
    spark = SparkSession.builder.appName("rfm").getOrCreate()
    try:
        df = spark.read.parquet(parquet_path)
    except AnalysisException as e:
        if "PATH_NOT_FOUND" in str(e):
            print(f"Error: The input parquet file '{parquet_path}' was not found.")
            print("Please ensure the file exists at this location or provide the correct path using the --input argument.")
            spark.stop()
            return  # Exit the function gracefully
        else:
            raise  # Re-raise other AnalysisExceptions

    # Aggregate per customer
    agg = (
        df.groupBy("CustomerID")
        .agg(
            _max("InvoiceDate").alias("LastPurchase"),
            _sum("Quantity").alias("Frequency"),
            _sum("TotalSale").alias("Monetary"),
        )
    )

    # Compute Recency as days since last purchase
    ref = lit(reference_date)
    agg = agg.withColumn("Recency", datediff(to_date(ref), to_date(col("LastPurchase"))))

    pdf = agg.toPandas()

    # Drop rows with NaN values in RFM columns before scoring
    pdf.dropna(subset=['Recency', 'Frequency', 'Monetary'], inplace=True)

    # Score (1-5) using quantiles dynamically based on actual bins created by qcut
    # Use labels=False to get integer codes (0-indexed) and dynamically determine score ranges.

    # Recency Score: higher score for lower recency (codes 0 to k-1 -> scores k to 1)
    r_qcut_result = pd.qcut(pdf['Recency'].rank(method='first'), 5, labels=False, duplicates='drop')
    r_num_bins = len(r_qcut_result.categories) if hasattr(r_qcut_result, 'categories') else 5 # Default to 5 if not Categorical
    pdf['R_score'] = (r_num_bins - r_qcut_result).astype(int)

    # Frequency Score: higher score for higher frequency (codes 0 to k-1 -> scores 1 to k)
    f_qcut_result = pd.qcut(pdf['Frequency'].rank(method='first'), 5, labels=False, duplicates='drop')
    f_num_bins = len(f_qcut_result.categories) if hasattr(f_qcut_result, 'categories') else 5
    pdf['F_score'] = (f_qcut_result + 1).astype(int)

    # Monetary Score: higher score for higher monetary value (codes 0 to k-1 -> scores 1 to k)
    m_qcut_result = pd.qcut(pdf['Monetary'].rank(method='first'), 5, labels=False, duplicates='drop')
    m_num_bins = len(m_qcut_result.categories) if hasattr(m_qcut_result, 'categories') else 5
    pdf['M_score'] = (m_qcut_result + 1).astype(int)

    pdf['RFM_Score'] = pdf['R_score']*100 + pdf['F_score']*10 + pdf['M_score']

    # Create reports directory if it doesn't exist
    Path(out_xlsx).parent.mkdir(parents=True, exist_ok=True)
    pdf.to_excel(out_xlsx, index=False)
    print(f"Saved RFM to {out_xlsx}")
    spark.stop()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', default='data/fact_retail.parquet')
    parser.add_argument('--out', default='reports/rfm_summary.xlsx')
    parser.add_argument('--ref', default='2025-11-24')
    args, unknown = parser.parse_known_args()
    compute_rfm(args.input, args.out, args.ref)

